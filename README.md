# Reverse Dictionary Using Deep Learning

 My two partners and I created an accurate reverse dictionary by creating a LSTM from scratch using TensorFlow Recurrent Neural Network cells. 
 In other words, our system takes in a string of words (i.e. an input phrase) that describe a word and outputs relevant words that best match the description in meaning. Instead of returning many possible words, we return top 10 words initially and also gauge the performance of top 3-5 words.
Word definitions were used from WordNet 3.0 by Princeton University. Some information included in WordNet 3.0 is word definitions, sample sentences, parts of speech information (e.g. noun, adjective, verb) and synonyms. Data cleansing is done in loadData.py, where the dictionary definitions are parsed into list of words and stored as a python dictionary with the corresponding words as keys. For testing we took phrases from Merriam-Webster dictionary and created a small random dataset to test performance and compare attention scores from different meanings of a word.

For vectorizing the words in the phrases, we used word embeddings from the pre-trained Wikipedia based GloVe dataset with 6 billion tokens in 50 dimensional vectors (as Wikipedia based dataset would be exhaustive and also descriptive as is a dictionary). Although we used several free trial versions of Microsoft Azure for training, training LSTM on 300d vectors of 840 billion tokens was logistically prohibitive. Using 50d GloVe embeddings enabled us to run the model locally as well (in addition to running them remotely) to compute results and test performance. 
We start with average embeddings as our baseline model and Long Short Term Memory based network as our final model.  LSTM layer is followed by upscaling dimension linear layer. 
